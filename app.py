import os
import torch
import re
from flask import Flask, request, jsonify, render_template, send_from_directory
from flask_cors import CORS
from werkzeug.utils import secure_filename
from PIL import Image
import io
import base64
from config import *
from transformers import AutoModel, AutoTokenizer

def format_ocr_result(text, output_format='markdown'):
    """Format OCR result v·ªõi layout ƒë·∫πp"""
    if not text:
        return text
    
    # N·∫øu c√≥ <|ref|> v√† <|det|> tags (grounding format)
    if '<|ref|>' in text and '<|det|>' in text:
        if output_format == 'markdown':
            # Convert sang markdown ƒë·∫πp
            return format_to_markdown(text)
        elif output_format == 'full':
            # Gi·ªØ nguy√™n format v·ªõi bounding boxes
            return format_with_boxes(text)
        else:
            # Ch·ªâ l·∫•y text, b·ªè tags
            return extract_text_only(text)
    else:
        # Kh√¥ng c√≥ tags, tr·∫£ v·ªÅ nguy√™n b·∫£n
        return text

def format_to_markdown(text):
    """Convert OCR result v·ªõi <|ref|> tags sang markdown"""
    lines = text.split('\n')
    markdown_lines = []
    current_section = None
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Parse <|ref|>tag<|/ref|><|det|>[[x1,y1,x2,y2]]<|/det|>
        ref_match = re.search(r'<\|ref\|>(.*?)<\|/ref\|>', line)
        det_match = re.search(r'<\|det\|>\[\[(.*?)\]\]<\|/det\|>', line)
        
        if ref_match:
            tag = ref_match.group(1)
            # L·∫•y text sau tags
            text_part = re.sub(r'<\|ref\|>.*?<\|/ref\|>', '', line)
            text_part = re.sub(r'<\|det\|>.*?<\|/det\|>', '', text_part).strip()
            
            # Format theo tag type
            if tag == 'sub_title' or tag == 'title':
                markdown_lines.append(f'\n## {text_part}\n')
            elif tag == 'text':
                markdown_lines.append(text_part)
            elif tag == 'image':
                markdown_lines.append(f'\n![Image]({text_part})\n')
            else:
                markdown_lines.append(f'**{tag}**: {text_part}')
        else:
            # Kh√¥ng c√≥ tags, th√™m text b√¨nh th∆∞·ªùng
            clean_line = re.sub(r'<\|.*?\|>', '', line).strip()
            if clean_line:
                markdown_lines.append(clean_line)
    
    return '\n'.join(markdown_lines)

def format_with_boxes(text):
    """Format v·ªõi bounding boxes info"""
    lines = text.split('\n')
    formatted_lines = []
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Parse tags
        ref_match = re.search(r'<\|ref\|>(.*?)<\|/ref\|>', line)
        det_match = re.search(r'<\|det\|>\[\[(.*?)\]\]<\|/det\|>', line)
        
        if ref_match and det_match:
            tag = ref_match.group(1)
            bbox = det_match.group(1)
            text_part = re.sub(r'<\|ref\|>.*?<\|/ref\|>', '', line)
            text_part = re.sub(r'<\|det\|>.*?<\|/det\|>', '', text_part).strip()
            
            formatted_lines.append(f'[{tag}] {text_part} | BBox: {bbox}')
        else:
            clean_line = re.sub(r'<\|.*?\|>', '', line).strip()
            if clean_line:
                formatted_lines.append(clean_line)
    
    return '\n'.join(formatted_lines)

def extract_text_only(text):
    """Ch·ªâ l·∫•y text, b·ªè t·∫•t c·∫£ tags"""
    # Remove all tags
    clean_text = re.sub(r'<\|.*?\|>', '', text)
    # Remove bounding boxes format
    clean_text = re.sub(r'\[\[.*?\]\]', '', clean_text)
    # Clean up multiple spaces
    clean_text = re.sub(r'\s+', ' ', clean_text)
    return clean_text.strip()

# Patch ƒë·ªÉ fix l·ªói DynamicCache.seen_tokens (transformers >= 4.41)
def patch_dynamic_cache():
    """Patch DynamicCache ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi transformers m·ªõi"""
    try:
        from transformers.cache_utils import DynamicCache
        import transformers
        
        # Ki·ªÉm tra version transformers
        version_parts = transformers.__version__.split('.')
        major, minor = int(version_parts[0]), int(version_parts[1])
        
        # N·∫øu transformers >= 4.41, c·∫ßn patch
        if major > 4 or (major == 4 and minor >= 41):
            # Th√™m thu·ªôc t√≠nh seen_tokens n·∫øu ch∆∞a c√≥
            if not hasattr(DynamicCache, 'seen_tokens'):
                def _get_seen_tokens(self):
                    """Get seen_tokens t·ª´ cache_position"""
                    try:
                        if hasattr(self, 'cache_position') and len(self.cache_position) > 0:
                            return len(self.cache_position)
                        elif hasattr(self, 'key_cache') and len(self.key_cache) > 0:
                            # Fallback: t√≠nh t·ª´ key_cache shape
                            return self.key_cache[0].shape[2] if len(self.key_cache) > 0 else 0
                    except:
                        pass
                    return 0
                
                DynamicCache.seen_tokens = property(_get_seen_tokens)
                
                # Th√™m method get_max_length n·∫øu ch∆∞a c√≥
                if not hasattr(DynamicCache, 'get_max_length'):
                    def _get_max_length(self):
                        """Get max length t·ª´ cache"""
                        try:
                            if hasattr(self, 'get_max_cache_shape'):
                                shape = self.get_max_cache_shape()
                                if shape and len(shape) > 1:
                                    return shape[1]
                        except:
                            pass
                        return None
                    DynamicCache.get_max_length = _get_max_length
                
                print("‚úÖ ƒê√£ patch DynamicCache ƒë·ªÉ t∆∞∆°ng th√≠ch v·ªõi transformers m·ªõi")
    except Exception as e:
        print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ patch DynamicCache: {e}")

# Ch·∫°y patch ngay khi import
patch_dynamic_cache()

app = Flask(__name__)
CORS(app)
app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = MAX_FILE_SIZE

# Global model and tokenizer
model = None
tokenizer = None

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

def init_model():
    """Initialize the DeepSeek-OCR model"""
    global model, tokenizer
    try:
        # Hi·ªÉn th·ªã th√¥ng tin c·∫•u h√¨nh
        print("=" * 60)
        print("C·∫•u h√¨nh h·ªá th·ªëng:")
        print(f"  - Device: {DEVICE}")
        print(f"  - Dtype: {DTYPE}")
        print(f"  - Image size: {IMAGE_SIZE}x{IMAGE_SIZE}")
        print(f"  - Base size: {BASE_SIZE}")
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"  - GPU: {gpu_name} ({gpu_memory:.1f}GB VRAM)")
        else:
            print("  - GPU: Kh√¥ng c√≥ (s·ª≠ d·ª•ng CPU)")
        print("=" * 60)
        
        print("\nƒêang t·∫£i tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            MODEL_NAME, 
            trust_remote_code=True
        )
        
        print("ƒêang t·∫£i model (c√≥ th·ªÉ m·∫•t v√†i ph√∫t l·∫ßn ƒë·∫ßu)...")
        print("L∆∞u √Ω: Model s·∫Ω ƒë∆∞·ª£c t·∫£i t·ª´ Hugging Face (~20-30GB)")
        
        # Ki·ªÉm tra v√† c√†i flash-attn n·∫øu c·∫ßn (ƒë·ªÉ c√≥ LlamaFlashAttention2)
        has_flash_attn = False
        try:
            import flash_attn
            has_flash_attn = True
            print("‚úÖ flash-attn ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t")
        except ImportError:
            print("‚ö†Ô∏è  flash-attn ch∆∞a ƒë∆∞·ª£c c√†i.")
            print("   ƒêang th·ª≠ t·∫£i model v·ªõi transformers m·∫∑c ƒë·ªãnh...")
        
        # Patch ƒë·ªÉ bypass flash attention n·∫øu c·∫ßn
        if not has_flash_attn:
            try:
                # Th·ª≠ import t·ª´ transformers tr∆∞·ªõc
                from transformers.models.llama import modeling_llama
                if not hasattr(modeling_llama, 'LlamaFlashAttention2'):
                    print("‚ö†Ô∏è  LlamaFlashAttention2 kh√¥ng c√≥ trong transformers.")
                    print("   ƒêang t·∫°o workaround...")
                    # T·∫°o class gi·∫£ ƒë·ªÉ model code kh√¥ng b·ªã l·ªói import
                    class FakeLlamaFlashAttention2:
                        def __init__(self, *args, **kwargs):
                            pass
                    modeling_llama.LlamaFlashAttention2 = FakeLlamaFlashAttention2
                    print("‚úÖ ƒê√£ t·∫°o workaround cho flash attention")
            except Exception as e:
                print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ patch: {e}")
        
        # Th·ª≠ load model
        try:
            # Kh√¥ng ch·ªâ ƒë·ªãnh _attn_implementation ƒë·ªÉ model t·ª± quy·∫øt ƒë·ªãnh
            model = AutoModel.from_pretrained(
                MODEL_NAME,
                trust_remote_code=True,
                use_safetensors=True
            )
        except Exception as e:
            error_msg = str(e)
            if "LlamaFlashAttention2" in error_msg or "flash" in error_msg.lower():
                print("\n" + "="*60)
                print("‚ö†Ô∏è  L·ªói li√™n quan ƒë·∫øn flash attention.")
                print("="*60)
                print("\nüí° Gi·∫£i ph√°p:")
                print("\n1. C√†i wheel v√† flash-attn:")
                print("   pip install wheel")
                print("   pip install flash-attn==2.7.3 --no-build-isolation")
                print("\n2. Ho·∫∑c c·∫≠p nh·∫≠t transformers:")
                print("   pip install --upgrade transformers>=4.51.0 accelerate")
                print("\n3. Ho·∫∑c c√†i t·ª´ pre-built wheel:")
                print("   pip install flash-attn --no-build-isolation")
                print("="*60)
                raise Exception(f"Model y√™u c·∫ßu flash-attn ho·∫∑c transformers m·ªõi h∆°n. L·ªói: {error_msg}")
            else:
                raise Exception(f"Kh√¥ng th·ªÉ t·∫£i model: {error_msg}")
        
        # Move to device and set dtype
        dtype_map = {
            'bfloat16': torch.bfloat16,
            'float16': torch.float16,
            'float32': torch.float32
        }
        dtype = dtype_map.get(DTYPE, torch.bfloat16)
        
        model = model.eval()
        if torch.cuda.is_available() and DEVICE == 'cuda':
            print(f"ƒêang chuy·ªÉn model l√™n GPU v·ªõi dtype={DTYPE}...")
            print("‚ö†Ô∏è  Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t 5-10 ph√∫t, vui l√≤ng ƒë·ª£i...")
            print("üí° ƒêang t·∫£i ~6.7GB weights l√™n GPU...")
            
            # Ki·ªÉm tra VRAM tr∆∞·ªõc khi t·∫£i
            torch.cuda.empty_cache()
            free_memory = torch.cuda.get_device_properties(0).total_memory - torch.cuda.memory_allocated(0)
            free_memory_gb = free_memory / (1024**3)
            print(f"üìä VRAM c√≤n tr·ªëng: {free_memory_gb:.1f}GB")
            
            if free_memory_gb < 8:
                print("‚ö†Ô∏è  VRAM h∆°i √≠t, c√≥ th·ªÉ m·∫•t nhi·ªÅu th·ªùi gian h∆°n...")
            
            # T·∫£i model l√™n GPU
            try:
                model = model.cuda()
                print("‚úÖ Model ƒë√£ ƒë∆∞·ª£c chuy·ªÉn l√™n GPU")
                print("üîÑ ƒêang chuy·ªÉn ƒë·ªïi dtype...")
                model = model.to(dtype)
                print("‚úÖ Dtype ƒë√£ ƒë∆∞·ª£c chuy·ªÉn ƒë·ªïi")
            except RuntimeError as e:
                if "out of memory" in str(e).lower():
                    print("‚ùå L·ªói: H·∫øt VRAM!")
                    print("üí° Gi·∫£i ph√°p: Gi·∫£m IMAGE_SIZE trong config.py ho·∫∑c d√πng CPU")
                    raise
                else:
                    raise
        else:
            print(f"ƒêang chuy·ªÉn model l√™n CPU v·ªõi dtype={DTYPE}...")
            print("‚ö†Ô∏è  Ch·∫°y tr√™n CPU s·∫Ω r·∫•t ch·∫≠m (30-60s/·∫£nh)...")
            model = model.to(dtype)
        
        # Ki·ªÉm tra model ƒë√£ s·∫µn s√†ng
        torch.cuda.empty_cache()
        if torch.cuda.is_available() and DEVICE == 'cuda':
            allocated = torch.cuda.memory_allocated(0) / (1024**3)
            print(f"üìä VRAM ƒë√£ s·ª≠ d·ª•ng: {allocated:.1f}GB")
        
        print("\n‚úÖ Model ƒë√£ ƒë∆∞·ª£c t·∫£i th√†nh c√¥ng!")
        print("=" * 60)
        return True
    except Exception as e:
        print(f"\n‚ùå L·ªói khi t·∫£i model: {str(e)}")
        print("\nG·ª£i √Ω kh·∫Øc ph·ª•c:")
        print("  1. Ki·ªÉm tra k·∫øt n·ªëi internet")
        print("  2. ƒê·∫£m b·∫£o c√≥ ƒë·ªß dung l∆∞·ª£ng ·ªï c·ª©ng (50GB+)")
        print("  3. Th·ª≠ ƒë·ªïi DEVICE='cpu' trong config.py n·∫øu GPU c√≥ v·∫•n ƒë·ªÅ")
        print("  4. Gi·∫£m IMAGE_SIZE trong config.py n·∫øu thi·∫øu RAM/VRAM")
        return False

@app.route('/')
def index():
    """Render the main page"""
    return render_template('index.html')

@app.route('/health')
def health():
    """Health check endpoint"""
    return jsonify({
        'status': 'ok',
        'model_loaded': model is not None,
        'device': DEVICE,
        'cuda_available': torch.cuda.is_available()
    })

@app.route('/api/ocr', methods=['POST'])
def ocr():
    """Process OCR request"""
    try:
        if model is None or tokenizer is None:
            return jsonify({
                'success': False,
                'error': 'Model ch∆∞a ƒë∆∞·ª£c t·∫£i. Vui l√≤ng ƒë·ª£i...'
            }), 503
        
        # Check if file is present
        if 'image' not in request.files:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng t√¨m th·∫•y file ·∫£nh'
            }), 400
        
        file = request.files['image']
        if file.filename == '':
            return jsonify({
                'success': False,
                'error': 'Ch∆∞a ch·ªçn file'
            }), 400
        
        if not allowed_file(file.filename):
            return jsonify({
                'success': False,
                'error': f'ƒê·ªãnh d·∫°ng file kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£. Ch·ªâ ch·∫•p nh·∫≠n: {", ".join(ALLOWED_EXTENSIONS)}'
            }), 400
        
        # Get prompt from form
        prompt_text = request.form.get('prompt', '').strip()
        output_format = request.form.get('format', 'markdown')  # markdown, text, full
        
        # Ch·ªçn prompt ph√π h·ª£p v·ªõi format
        if not prompt_text:
            if output_format == 'markdown':
                prompt_text = '<image>\n<|grounding|>Convert the document to markdown.'
            elif output_format == 'full':
                prompt_text = '<image>\n<|grounding|>OCR this image.'
            else:
                prompt_text = '<image>\nFree OCR.'
        
        # Read image
        image_bytes = file.read()
        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
        
        # Save uploaded file
        filename = secure_filename(file.filename)
        filepath = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        image.save(filepath)
        
        # Process OCR
        output_path = os.path.join(OUTPUT_FOLDER, f"result_{filename}")
        
        # Gi·∫£m k√≠ch th∆∞·ªõc ·∫£nh n·∫øu qu√° l·ªõn ƒë·ªÉ tr√°nh l·ªói CUDA
        actual_image_size = IMAGE_SIZE
        if image.size[0] > 2048 or image.size[1] > 2048:
            # Resize ·∫£nh l·ªõn xu·ªëng
            max_dim = max(image.size)
            if max_dim > 2048:
                scale = 2048 / max_dim
                new_size = (int(image.size[0] * scale), int(image.size[1] * scale))
                image = image.resize(new_size, Image.Resampling.LANCZOS)
                image.save(filepath)  # L∆∞u l·∫°i ·∫£nh ƒë√£ resize
                print(f"‚ö†Ô∏è  ·∫¢nh qu√° l·ªõn, ƒë√£ resize t·ª´ {image.size} xu·ªëng {new_size}")
        
        try:
            # Th·ª≠ v·ªõi image_size nh·ªè h∆°n n·∫øu g·∫∑p l·ªói
            result = model.infer(
                tokenizer,
                prompt=prompt_text,
                image_file=filepath,
                output_path=output_path,
                base_size=BASE_SIZE,
                image_size=min(actual_image_size, 640),  # Gi·ªõi h·∫°n t·ªëi ƒëa 640
                crop_mode=CROP_MODE,
                save_results=True,
                test_compress=False  # T·∫Øt test_compress ƒë·ªÉ tr√°nh l·ªói
            )
        except RuntimeError as e:
            error_str = str(e)
            if "masked_scatter" in error_str or "CUDA" in error_str:
                # Th·ª≠ l·∫°i v·ªõi image_size nh·ªè h∆°n
                print(f"‚ö†Ô∏è  L·ªói CUDA v·ªõi image_size={actual_image_size}, th·ª≠ l·∫°i v·ªõi 512...")
                try:
                    result = model.infer(
                        tokenizer,
                        prompt=prompt_text,
                        image_file=filepath,
                        output_path=output_path,
                        base_size=768,
                        image_size=512,
                        crop_mode=CROP_MODE,
                        save_results=True,
                        test_compress=False
                    )
                except Exception as e2:
                    raise Exception(f"L·ªói khi x·ª≠ l√Ω OCR (ƒë√£ th·ª≠ gi·∫£m k√≠ch th∆∞·ªõc): {str(e2)}")
            else:
                raise Exception(f"L·ªói khi x·ª≠ l√Ω OCR: {error_str}")
        except Exception as e:
            raise Exception(f"L·ªói khi x·ª≠ l√Ω OCR: {str(e)}")
        
        # Read result - try multiple methods
        result_text = ""
        
        # Method 1: Try to read from output file (model.infer saves to file)
        output_dir = OUTPUT_FOLDER
        possible_files = [
            f"{output_path}.txt",
            f"{output_path}",
            os.path.join(output_dir, f"result_{filename}.txt"),
            os.path.join(output_dir, f"result_{filename}"),
        ]
        
        # T√¨m file m·ªõi nh·∫•t trong output_dir c√≥ ch·ª©a filename
        if os.path.exists(output_dir):
            all_files = []
            for f in os.listdir(output_dir):
                if filename in f or "result_" in f:
                    filepath_full = os.path.join(output_dir, f)
                    if os.path.isfile(filepath_full):
                        all_files.append((filepath_full, os.path.getmtime(filepath_full)))
            
            if all_files:
                # S·∫Øp x·∫øp theo th·ªùi gian, l·∫•y file m·ªõi nh·∫•t
                all_files.sort(key=lambda x: x[1], reverse=True)
                possible_files.insert(0, all_files[0][0])
        
        # Th·ª≠ ƒë·ªçc t·ª´ c√°c file c√≥ th·ªÉ
        for file_path in possible_files:
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        if content and len(content) > 10:  # ƒê·∫£m b·∫£o c√≥ n·ªôi dung
                            result_text = content
                            print(f"‚úÖ ƒê√£ ƒë·ªçc k·∫øt qu·∫£ t·ª´: {file_path}")
                            break
                except Exception as e:
                    print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ƒë·ªçc file {file_path}: {e}")
                    continue
        
        # Method 2: Try to get from result object
        if not result_text and result is not None:
            if isinstance(result, dict):
                result_text = result.get('text', result.get('result', result.get('output', str(result))))
            elif isinstance(result, str):
                result_text = result
            elif hasattr(result, 'text'):
                result_text = result.text
            elif hasattr(result, 'output'):
                result_text = result.output
            else:
                result_text = str(result) if result else ""
        
        # Method 3: N·∫øu v·∫´n kh√¥ng c√≥, t√¨m trong OUTPUT_FOLDER file m·ªõi nh·∫•t
        if not result_text and os.path.exists(OUTPUT_FOLDER):
            try:
                files = [f for f in os.listdir(OUTPUT_FOLDER) if os.path.isfile(os.path.join(OUTPUT_FOLDER, f))]
                if files:
                    # L·∫•y file m·ªõi nh·∫•t
                    latest_file = max(files, key=lambda f: os.path.getmtime(os.path.join(OUTPUT_FOLDER, f)))
                    latest_path = os.path.join(OUTPUT_FOLDER, latest_file)
                    with open(latest_path, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        if content:
                            result_text = content
                            print(f"‚úÖ ƒê√£ ƒë·ªçc k·∫øt qu·∫£ t·ª´ file m·ªõi nh·∫•t: {latest_file}")
            except Exception as e:
                print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ƒë·ªçc file m·ªõi nh·∫•t: {e}")
        
        # Clean up v√† format result text
        if result_text:
            result_text = result_text.strip()
            
            # Parse v√† format k·∫øt qu·∫£ ƒë·∫πp h∆°n n·∫øu c√≥ <|ref|> v√† <|det|> tags
            if '<|ref|>' in result_text or '<|det|>' in result_text:
                # Format v·ªõi layout info
                formatted_text = format_ocr_result(result_text, output_format)
            else:
                formatted_text = result_text
        else:
            formatted_text = "Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£. Vui l√≤ng ki·ªÉm tra logs."
            print(f"‚ö†Ô∏è  Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£. Output path: {output_path}")
            print(f"‚ö†Ô∏è  Files trong OUTPUT_FOLDER: {os.listdir(OUTPUT_FOLDER) if os.path.exists(OUTPUT_FOLDER) else 'Kh√¥ng t·ªìn t·∫°i'}")
        
        return jsonify({
            'success': True,
            'text': formatted_text,
            'raw_text': result_text if result_text else "",
            'filename': filename,
            'format': output_format
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'L·ªói x·ª≠ l√Ω: {str(e)}'
        }), 500

@app.route('/api/ocr-base64', methods=['POST'])
def ocr_base64():
    """Process OCR from base64 image"""
    try:
        if model is None or tokenizer is None:
            return jsonify({
                'success': False,
                'error': 'Model ch∆∞a ƒë∆∞·ª£c t·∫£i. Vui l√≤ng ƒë·ª£i...'
            }), 503
        
        data = request.get_json()
        if not data or 'image' not in data:
            return jsonify({
                'success': False,
                'error': 'Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ·∫£nh'
            }), 400
        
        # Decode base64 image
        image_data = data['image']
        if image_data.startswith('data:image'):
            image_data = image_data.split(',')[1]
        
        image_bytes = base64.b64decode(image_data)
        image = Image.open(io.BytesIO(image_bytes)).convert('RGB')
        
        # Get prompt
        prompt_text = data.get('prompt', '<image>\nFree OCR.')
        if not prompt_text.strip():
            prompt_text = '<image>\nFree OCR.'
        
        # Save temporary file
        import uuid
        temp_filename = f"temp_{uuid.uuid4().hex}.png"
        temp_filepath = os.path.join(app.config['UPLOAD_FOLDER'], temp_filename)
        image.save(temp_filepath)
        
        # Process OCR
        output_path = os.path.join(OUTPUT_FOLDER, f"result_{temp_filename}")
        
        # Gi·∫£m k√≠ch th∆∞·ªõc ·∫£nh n·∫øu qu√° l·ªõn
        if image.size[0] > 2048 or image.size[1] > 2048:
            max_dim = max(image.size)
            if max_dim > 2048:
                scale = 2048 / max_dim
                new_size = (int(image.size[0] * scale), int(image.size[1] * scale))
                image = image.resize(new_size, Image.Resampling.LANCZOS)
                image.save(temp_filepath)
                print(f"‚ö†Ô∏è  ·∫¢nh qu√° l·ªõn, ƒë√£ resize t·ª´ {image.size} xu·ªëng {new_size}")
        
        try:
            result = model.infer(
                tokenizer,
                prompt=prompt_text,
                image_file=temp_filepath,
                output_path=output_path,
                base_size=BASE_SIZE,
                image_size=min(IMAGE_SIZE, 640),  # Gi·ªõi h·∫°n t·ªëi ƒëa 640
                crop_mode=CROP_MODE,
                save_results=True,
                test_compress=False  # T·∫Øt test_compress ƒë·ªÉ tr√°nh l·ªói
            )
        except RuntimeError as e:
            error_str = str(e)
            if "masked_scatter" in error_str or "CUDA" in error_str:
                print(f"‚ö†Ô∏è  L·ªói CUDA, th·ª≠ l·∫°i v·ªõi image_size nh·ªè h∆°n...")
                try:
                    result = model.infer(
                        tokenizer,
                        prompt=prompt_text,
                        image_file=temp_filepath,
                        output_path=output_path,
                        base_size=768,
                        image_size=512,
                        crop_mode=CROP_MODE,
                        save_results=True,
                        test_compress=False
                    )
                except Exception as e2:
                    raise Exception(f"L·ªói khi x·ª≠ l√Ω OCR (ƒë√£ th·ª≠ gi·∫£m k√≠ch th∆∞·ªõc): {str(e2)}")
            else:
                raise Exception(f"L·ªói khi x·ª≠ l√Ω OCR: {error_str}")
        except Exception as e:
            raise Exception(f"L·ªói khi x·ª≠ l√Ω OCR: {str(e)}")
        
        # Read result - try multiple methods (same as ocr function)
        result_text = ""
        
        # Method 1: Try to read from output file
        possible_files = [
            f"{output_path}.txt",
            f"{output_path}",
            os.path.join(OUTPUT_FOLDER, f"result_{temp_filename}.txt"),
            os.path.join(OUTPUT_FOLDER, f"result_{temp_filename}"),
        ]
        
        # T√¨m file m·ªõi nh·∫•t
        if os.path.exists(OUTPUT_FOLDER):
            all_files = []
            for f in os.listdir(OUTPUT_FOLDER):
                if temp_filename in f or "result_" in f:
                    filepath_full = os.path.join(OUTPUT_FOLDER, f)
                    if os.path.isfile(filepath_full):
                        all_files.append((filepath_full, os.path.getmtime(filepath_full)))
            
            if all_files:
                all_files.sort(key=lambda x: x[1], reverse=True)
                possible_files.insert(0, all_files[0][0])
        
        for file_path in possible_files:
            if os.path.exists(file_path):
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read().strip()
                        if content and len(content) > 10:
                            result_text = content
                            break
                except:
                    continue
        
        # Method 2: Try result object
        if not result_text and result is not None:
            if isinstance(result, dict):
                result_text = result.get('text', result.get('result', result.get('output', str(result))))
            elif isinstance(result, str):
                result_text = result
            elif hasattr(result, 'text'):
                result_text = result.text
            elif hasattr(result, 'output'):
                result_text = result.output
            else:
                result_text = str(result) if result else ""
        
        if result_text:
            result_text = result_text.strip()
        
        # Clean up temp file
        try:
            os.remove(temp_filepath)
        except:
            pass
        
        return jsonify({
            'success': True,
            'text': result_text
        })
        
    except Exception as e:
        return jsonify({
            'success': False,
            'error': f'L·ªói x·ª≠ l√Ω: {str(e)}'
        }), 500

if __name__ == '__main__':
    print("=" * 50)
    print("ƒêang kh·ªüi t·∫°o DeepSeek-OCR Web Application...")
    print("=" * 50)
    
    # Initialize model
    if init_model():
        print(f"\nServer ƒëang ch·∫°y t·∫°i: http://{HOST}:{PORT}")
        print("Nh·∫•n Ctrl+C ƒë·ªÉ d·ª´ng server\n")
        app.run(host=HOST, port=PORT, debug=DEBUG)
    else:
        print("Kh√¥ng th·ªÉ kh·ªüi t·∫°o model. Vui l√≤ng ki·ªÉm tra l·∫°i c·∫•u h√¨nh.")

