# ðŸ”§ Kháº¯c phá»¥c lá»—i transformers

## Lá»—i: "cannot import name 'LlamaFlashAttention2'"

### NguyÃªn nhÃ¢n:
- Version `transformers` khÃ´ng tÆ°Æ¡ng thÃ­ch vá»›i model
- Model cáº§n version transformers cá»¥ thá»ƒ

### Giáº£i phÃ¡p:

**TrÃªn server Linux, cháº¡y:**

```bash
# Äáº£m báº£o Ä‘ang trong virtual environment
source venv/bin/activate

# Cáº­p nháº­t transformers lÃªn version tÆ°Æ¡ng thÃ­ch
pip install --upgrade "transformers>=4.46.0,<5.0.0"

# Hoáº·c cÃ i láº¡i táº¥t cáº£ tá»« requirements.txt (Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t)
pip install -r requirements.txt
```

**Sau Ä‘Ã³ cháº¡y láº¡i:**
```bash
python app.py
```

---

## Náº¿u váº«n lá»—i:

### CÃ¡ch 1: CÃ i flash-attn (Khuyáº¿n nghá»‹ náº¿u cÃ³ GPU)

```bash
pip install flash-attn==2.7.3 --no-build-isolation
```

### CÃ¡ch 2: Cáº­p nháº­t transformers lÃªn version má»›i nháº¥t

```bash
pip install --upgrade transformers accelerate
```

### CÃ¡ch 3: Kiá»ƒm tra version hiá»‡n táº¡i

```bash
python -c "import transformers; print(transformers.__version__)"
```

Version khuyáº¿n nghá»‹: **4.46.0 - 4.51.x**

---

## LÆ°u Ã½:

- Model DeepSeek-OCR yÃªu cáº§u `trust_remote_code=True` (báº¯t buá»™c)
- KhÃ´ng cáº§n flash_attention_2 Ä‘á»ƒ cháº¡y, chá»‰ cáº§n Ä‘á»ƒ tÄƒng tá»‘c
- Code Ä‘Ã£ Ä‘Æ°á»£c cáº­p nháº­t Ä‘á»ƒ tá»± Ä‘á»™ng xá»­ lÃ½ trÆ°á»ng há»£p khÃ´ng cÃ³ flash attention

